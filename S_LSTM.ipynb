{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XCZgG1WAsqysR3bGoB1pshvvocir9P6Z",
      "authorship_tag": "ABX9TyOwoTfIjmwShOOAFxWhkpZ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabboM/NNDS/blob/master/S_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um7oKpoM5PwK",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks for Data Science Applications\n",
        "\n",
        "Code and work is related to this [Paper](https://arxiv.org/pdf/1805.02474.pdf)\n",
        "and some code is adapted from [here](https://keras.io/examples/nlp/pretrained_word_embeddings/) and [here](https://medium.com/softmax/tensorflow-keras-lstm-source-code-line-by-line-explained-125a6dae0622)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soQVjZmolB8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.keras.activations import sigmoid\n",
        "from tensorflow.keras.activations import tanh\n",
        "\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJeBavA453kc",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mff73X42QyUh",
        "colab_type": "text"
      },
      "source": [
        "### Loading IMDB_reviews and splitting in Train/Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPFM_ExG5b2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews/plain_text',\n",
        "                                          split=['train', 'test'],\n",
        "                                          shuffle_files=True,\n",
        "                                          as_supervised=True,\n",
        "                                          with_info=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgBAsKeLRKX8",
        "colab_type": "text"
      },
      "source": [
        "creating a list `ds` of all the reviews in plain text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlxnM6v28NVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "it = list(ds_train)\n",
        "ds = []\n",
        "for i in it:\n",
        "  ds.append(i[0].numpy().decode())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHeNNRoXgkAp",
        "colab_type": "text"
      },
      "source": [
        "Let's download the GloVe word embeddings. We will use dim=100 instead of 300 to speed up the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKlCN0BJYUW4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "cf89eec1-0433-4a31-c873-b8b3aae589c4"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-05 12:49:50--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-09-05 12:49:50--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-09-05 12:49:50--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.2’\n",
            "\n",
            "glove.6B.zip.2      100%[===================>] 822.24M  2.18MB/s    in 6m 27s  \n",
            "\n",
            "2020-09-05 12:56:17 (2.13 MB/s) - ‘glove.6B.zip.2’ saved [862182613/862182613]\n",
            "\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBwB_-ixkzhj",
        "colab_type": "text"
      },
      "source": [
        "creating a mapping of the words to their vector representation by GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NVdXHklkwiq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "62a9cc77-80e0-4f01-dcf7-b3814c9eff50"
      },
      "source": [
        "path_to_glove_file = os.path.join(\n",
        "    \"glove.6B.100d.txt\"\n",
        ")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFtRkRTOSBeb",
        "colab_type": "text"
      },
      "source": [
        "Now we should have a tokenizer for the raw text. We train a word tokenizer on the training corpus and create a dictionary with the corresponding vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0E1mmFxzq83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_dataset = tf.data.Dataset.from_tensor_slices(ds)\n",
        "max_features = 20000  # Maximum vocab size.\n",
        "max_len = 200  # Sequence length to pad the outputs to.\n",
        "\n",
        "# Create the layer.\n",
        "vectorize = TextVectorization(\n",
        " max_tokens=max_features,\n",
        " output_mode='int',\n",
        " output_sequence_length=max_len)\n",
        "\n",
        "# Now that the vocab layer has been created, call `adapt` on the text-only\n",
        "# dataset to create the vocabulary. You don't have to batch, but for large\n",
        "# datasets this means we're not keeping spare copies of the dataset.\n",
        "vectorize.adapt(text_dataset.batch(64))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKx0HkfC9m-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voc = vectorize.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS5m-YzFSoV_",
        "colab_type": "text"
      },
      "source": [
        "creating an embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CouWkFjo9Hq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b0248ab6-34b3-479e-a425-65c20485683d"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converted 18723 words (1277 misses)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVPwSnBA9yaC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "88a8703f-f02a-4918-d6ba-afb8a3dd8c1b"
      },
      "source": [
        "len(embeddings_index.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMAaLEbXTag_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "26901f0e-2775-4c31-93a8-ab4f76f21317"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20002, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCQHcJM_cR3A",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAmh_RCv2qzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CBmhZpe7l0q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a610b359-145f-439a-aa3a-58c338c15a40"
      },
      "source": [
        "class SLSTMcell(keras.layers.RNN):\n",
        "    def __init__(self, units=32, window=3, use_bias=True, recurrent_fn=sigmoid, activation_fn=tanh, seq_len=max_len):\n",
        "        super(SLTMcell, self).__init__()\n",
        "        self.units = units\n",
        "        self.use_bias = bias\n",
        "        self.recurrent_function = recurrent_fn\n",
        "        self.activation_fn = activation_fn\n",
        "        self.seq_len = seq_len\n",
        "      \n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_dim * window, self.units * 7),\n",
        "                                      name='kernel',\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        \n",
        "        self.U = self.add_weight(shape=(input_dim, self.units * 7),\n",
        "                                      name='kernel',\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        \n",
        "        self.V = self.add_weight(shape=(input_dim, self.units * 7),\n",
        "                                      name='kernel',\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "\n",
        "        if self.use_bias:\n",
        "            # bias_initializer = self.bias_initializer\n",
        "            self.bias = self.add_weight(shape=(self.units * 7,),\n",
        "                                        name='bias',\n",
        "                                        initializer=bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        self.W_i = self.W[:, :self.units]\n",
        "        self.W_l = self.W[:, self.units * 1: self.units * 2]\n",
        "        self.W_r = self.W[:, self.units * 2: self.units * 3]\n",
        "        self.W_f = self.W[:, self.units * 3: self.units * 4]\n",
        "        self.W_s = self.W[:, self.units * 4: self.units * 5]\n",
        "        self.W_o = self.W[:, self.units * 5: self.units * 6]\n",
        "        self.W_u = self.W[:, self.units * 6:]\n",
        "        \n",
        "        self.U_i = self.U[:, :self.units]\n",
        "        self.U_l = self.U[:, self.units * 1: self.units * 2]\n",
        "        self.U_r = self.U[:, self.units * 2: self.units * 3]\n",
        "        self.U_f = self.U[:, self.units * 3: self.units * 4]\n",
        "        self.U_s = self.U[:, self.units * 4: self.units * 5]\n",
        "        self.U_o = self.U[:, self.units * 5: self.units * 6]\n",
        "        self.U_u = self.U[:, self.units * 6:]\n",
        "        \n",
        "        self.V_i = self.V[:, :self.units]\n",
        "        self.V_l = self.V[:, self.units * 1: self.units * 2]\n",
        "        self.V_r = self.V[:, self.units * 2: self.units * 3]\n",
        "        self.V_f = self.V[:, self.units * 3: self.units * 4]\n",
        "        self.V_s = self.V[:, self.units * 4: self.units * 5]\n",
        "        self.V_o = self.V[:, self.units * 5: self.units * 6]\n",
        "        self.V_u = self.V[:, self.units * 6:]\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias_i = self.bias[:self.units]\n",
        "            self.bias_l = self.bias[self.units * 1: self.units * 2]\n",
        "            self.bias_r = self.bias[self.units * 2: self.units * 3]\n",
        "            self.bias_f = self.bias[self.units * 3: self.units * 4]\n",
        "            self.bias_s = self.bias[self.units * 4: self.units * 5]\n",
        "            self.bias_o = self.bias[self.units * 5: self.units * 6]\n",
        "            self.bias_u = self.bias[self.units * 6:]\n",
        "        else:\n",
        "            self.bias_i = None\n",
        "            self.bias_l = None\n",
        "            self.bias_r = None\n",
        "            self.bias_f = None\n",
        "            self.bias_s = None\n",
        "            self.bias_o = None\n",
        "            self.bias_u = None\n",
        "        self.built = True\n",
        "\n",
        "   def call(self, inputs, states, training=None):\n",
        "\n",
        "        H_tm1 = states[:, :self.seq_len + 2]  # previous memory state\n",
        "        c_tm1 = states[:, self.seq_len + 2:]  # previous carry state\n",
        "\n",
        "        for i in range(1, self.seq_len + 1):\n",
        "            x_i = inputs[i]\n",
        "            x_l = inputs[i]\n",
        "            x_r = inputs[i]\n",
        "            x_f = inputs[i]\n",
        "            x_s = inputs[i]\n",
        "            x_o = inputs[i]\n",
        "            x_u = inputs[i]\n",
        "            Ux_i = K.dot(x_i, self.U_i)\n",
        "            Ux_l = K.dot(x_l, self.U_l)\n",
        "            Ux_r = K.dot(x_r, self.U_r)\n",
        "            Ux_f = K.dot(x_f, self.U_f)\n",
        "            Ux_s = K.dot(x_s, self.U_s)\n",
        "            Ux_o = K.dot(x_o, self.U_o)\n",
        "            Ux_u = K.dot(x_u, self.U_u)\n",
        "            if self.use_bias:\n",
        "                Ux_i = K.bias_add(Ux_i, self.bias_i)\n",
        "                Ux_l = K.bias_add(Ux_l, self.bias_l)\n",
        "                Ux_r = K.bias_add(Ux_r, self.bias_r)\n",
        "                Ux_f = K.bias_add(Ux_f, self.bias_f)\n",
        "                Ux_s = K.bias_add(Ux_s, self.bias_s)\n",
        "                Ux_o = K.bias_add(Ux_o, self.bias_o)\n",
        "                Ux_u = K.bias_add(Ux_u, self.bias_u)\n",
        "\n",
        "            \n",
        "            csi_tm1_i = tf.concat((H_tm1[:, i-1], H_tm1[:, i], H_tm1[:, i+1]), axis=0)\n",
        "            csi_tm1_l = tf.concat((H_tm1[:, i-1], H_tm1[:, i], H_tm1[:, i+1]), axis=0)\n",
        "            csi_tm1_r = tf.concat((H_tm1[:, i-1], H_tm1[:, i], H_tm1[:, i+1]), axis=0)\n",
        "            csi_tm1_f = tf.concat((H_tm1[:, i-1], H_tm1[:, i], H_tm1[:, i+1]), axis=0)\n",
        "            csi_tm1_s = tf.concat((H_tm1[:, i-1], H_tm1[:, i], H_tm1[:, i+1]), axis=0)\n",
        "            csi_tm1_o = tf.concat((H_tm1[:, i-1], H_tm1[:, i], H_tm1[:, i+1]), axis=0)\n",
        "            csi_tm1_u = tf.concat((H_tm1[:, i-1], H_tm1[:, i], H_tm1[:, i+1]), axis=0)\n",
        "            Wcsi_i = K.dot(h_tm1_i, self.W_i)\n",
        "            Wcsi_l = K.dot(h_tm1_l, self.W_l)\n",
        "            Wcsi_r = K.dot(h_tm1_r, self.W_r)\n",
        "            Wcsi_f = K.dot(h_tm1_f, self.W_f)\n",
        "            Wcsi_s = K.dot(h_tm1_s, self.W_s)\n",
        "            Wcsi_o = K.dot(h_tm1_o, self.W_o)\n",
        "            Wcsi_u = K.dot(h_tm1_u, self.W_u)\n",
        "            \n",
        "            g_tm1_i = H_tm1[:, self.seq_len + 3]\n",
        "            g_tm1_l = H_tm1[:, self.seq_len + 3]\n",
        "            g_tm1_r = H_tm1[:, self.seq_len + 3]\n",
        "            g_tm1_f = H_tm1[:, self.seq_len + 3]\n",
        "            g_tm1_s = H_tm1[:, self.seq_len + 3]\n",
        "            g_tm1_o = H_tm1[:, self.seq_len + 3]\n",
        "            g_tm1_u = H_tm1[:, self.seq_len + 3]\n",
        "            Vg_i = K.dot(g_tm1_i, self.V_i)\n",
        "            Vg_l = K.dot(g_tm1_l, self.V_l)\n",
        "            Vg_r = K.dot(g_tm1_r, self.V_r)\n",
        "            Vg_f = K.dot(g_tm1_f, self.V_f)\n",
        "            Vg_s = K.dot(g_tm1_s, self.V_s)\n",
        "            Vg_o = K.dot(g_tm1_o, self.V_o)\n",
        "            Vg_u = K.dot(g_tm1_u, self.V_u)\n",
        "\n",
        "            i_hat = self.recurrent_activation(Wcsi_i + Ux_i + Vg_i)\n",
        "            l_hat = self.recurrent_activation(Wcsi_l + Ux_l + Vg_l)\n",
        "            r_hat = self.recurrent_activation(Wcsi_r + Ux_r + Vg_r)\n",
        "            f_hat = self.recurrent_activation(Wcsi_f + Ux_f + Vg_f)\n",
        "            s_hat = self.recurrent_activation(Wcsi_s + Ux_s + Vg_s)\n",
        "\n",
        "            \n",
        "\n",
        "            c_hat = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1_c,\n",
        "                                                            self.recurrent_kernel_c))\n",
        "            o_hat = self.recurrent_activation(x_o + K.dot(h_tm1_o,\n",
        "                                                      self.recurrent_kernel_o))\n",
        "      \n",
        "        h = o * self.activation(c)\n",
        "        return h, [h, c]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM1im9tHmgOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.constant([1,2,3], shape=(1,3))\n",
        "W = tf.transpose(tf.constant([[1,1,1],[2,2,2]], dtype=tf.float64))\n",
        "Y = W*2"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j99xXQ4Omm0V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "0e07a37e-e05e-47fb-ef30-c56f1d2929bc"
      },
      "source": [
        "K.dot(x,W)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-841cd0000979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1829\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1831\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1832\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   3253\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3254\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 3255\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   3256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5622\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5623\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5624\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5625\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5626\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute MatMul as input #1(zero-based) was expected to be a int32 tensor but is a half tensor [Op:MatMul]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxREQ6p7moeT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "012c225f-c8af-4a47-e8d2-7b2738ee15bd"
      },
      "source": [
        "x = tf.constant([1,2,3], shape=(3,))\n",
        "x"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4iaEbA3UKdd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e40e14c3-ef63-474b-b392-1b99d1c9c593"
      },
      "source": [
        "l = list(range(20))\n",
        "print(l[:12],l[12:])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] [12, 13, 14, 15, 16, 17, 18, 19]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VECJLYFoFAeM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6d3bcf0b-cc5a-44ed-a9a8-b0206f8c600d"
      },
      "source": [
        "tf.concat((x,x), axis=0)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzJPBMKEJMbF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "dcf16f12-60ba-48a8-9c7d-58ac65b568d5"
      },
      "source": [
        "tf.keras.activations.softmax(\n",
        "    W, axis=-1\n",
        ")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 2), dtype=float64, numpy=\n",
              "array([[0.26894142, 0.73105858],\n",
              "       [0.26894142, 0.73105858],\n",
              "       [0.26894142, 0.73105858]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLvI3WkgMz-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}
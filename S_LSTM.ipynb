{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XCZgG1WAsqysR3bGoB1pshvvocir9P6Z",
      "authorship_tag": "ABX9TyNyCGfXW0+/PO0OaQMYFcoB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabboM/NNDS/blob/master/S_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um7oKpoM5PwK",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks for Data Science Applications\n",
        "\n",
        "Code and work is related to this [Paper](https://arxiv.org/pdf/1805.02474.pdf)\n",
        "and some code is adapted from [here](https://keras.io/examples/nlp/pretrained_word_embeddings/) and [here](https://medium.com/softmax/tensorflow-keras-lstm-source-code-line-by-line-explained-125a6dae0622)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soQVjZmolB8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.keras.activations import sigmoid, tanh\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras import Sequential, losses, optimizers, metrics\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJeBavA453kc",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mff73X42QyUh",
        "colab_type": "text"
      },
      "source": [
        "### Loading IMDB_reviews and splitting in Train/Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPFM_ExG5b2q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d204e397-6b1b-42d3-e4d7-f4c321d6d2bc"
      },
      "source": [
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews/plain_text',\n",
        "                                          split=['train', 'test'],\n",
        "                                          shuffle_files=True,\n",
        "                                          as_supervised=True,\n",
        "                                          with_info=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0df3282c7fca4b28ada8ae3c8eb0f950",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2329c56969d44edf9b740dc14453f599",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2547aaff795484f824d26f5b5682a0d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rShuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete60XQPD/imdb_reviews-train.tfrecord\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a241f750eb284f10b94677ec5f849918",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0713734150674f4f80aa06724cbd049e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rShuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete60XQPD/imdb_reviews-test.tfrecord\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "631edff5cf0c4d99be52a23d64552425",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bdb88f9e9fb4dd99d28a5e506df61ec",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgBAsKeLRKX8",
        "colab_type": "text"
      },
      "source": [
        "creating a list `ds` of all the reviews in plain text. It will be used to train the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlxnM6v28NVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "it = list(ds_train)\n",
        "ds = []\n",
        "for _ in range(10000): #manual tokens\n",
        "  ds.append('startofsentence endofsentence')\n",
        "for i in it:\n",
        "  ds.append(i[0].numpy().decode())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHeNNRoXgkAp",
        "colab_type": "text"
      },
      "source": [
        "Let's download the GloVe word embeddings. We will use dim=50 instead of 300 to speed up the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKlCN0BJYUW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip -q glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBwB_-ixkzhj",
        "colab_type": "text"
      },
      "source": [
        "creating a mapping of the words to their vector representation by GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NVdXHklkwiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_glove_file = os.path.join(\n",
        "    \"drive/My Drive/NNDS/glove.6B.50d.txt\"\n",
        ")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFtRkRTOSBeb",
        "colab_type": "text"
      },
      "source": [
        "Now we should have a tokenizer for the raw text. We train a word tokenizer on the training corpus and create a dictionary with the corresponding vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0E1mmFxzq83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_dataset = tf.data.Dataset.from_tensor_slices(ds)\n",
        "max_features =5000  # Maximum vocab size.\n",
        "max_len = 50  # Sequence length to pad the outputs to.\n",
        "\n",
        "# Create the layer.\n",
        "vectorize = TextVectorization(\n",
        " max_tokens=max_features,\n",
        " output_mode='int',\n",
        " output_sequence_length=max_len)\n",
        "\n",
        "# Now that the vocab layer has been created, call `adapt` on the text-only\n",
        "# dataset to create the vocabulary. You don't have to batch, but for large\n",
        "# datasets this means we're not keeping spare copies of the dataset.\n",
        "vectorize.adapt(text_dataset.batch(64))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKx0HkfC9m-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voc = vectorize.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpWAJ1yvdFRa",
        "colab_type": "text"
      },
      "source": [
        "manually adding \\<begin of sentence\\> and \\<end of sentence\\> tokens and defining a function that we will use later on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6TJt1Lw7Hw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('startofsentence' in voc)\n",
        "print('endofsentence' in voc)\n",
        "\n",
        "def preprocess_sentence(x):\n",
        "  return('startofsentence ' + x + ' endofsentence')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS5m-YzFSoV_",
        "colab_type": "text"
      },
      "source": [
        "creating an embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CouWkFjo9Hq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 50\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAzg3jtxgPn1",
        "colab_type": "text"
      },
      "source": [
        "## Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAnEbTJagOnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_samples = []\n",
        "train_labels = []\n",
        "for i in ds_train:\n",
        "  train_samples.append(i[0].numpy().decode('utf-8'))\n",
        "  train_labels.append(i[1].numpy())\n",
        "\n",
        "x_train = vectorize(np.array([[preprocess_sentence(sent)] for sent in train_samples])).numpy()[:1000]\n",
        "y_train = np.array(train_labels)[:1000]\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCQHcJM_cR3A",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAmh_RCv2qzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFpdwUah6fcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SLSTMcellold(keras.layers.Layer):\n",
        "  def __init__(self, units=50, window=3, use_bias=True, recurrent_fn=sigmoid, activation_fn=tanh, seq_len=max_len, \n",
        "               kernel_initializer='uniform', kernel_regularizer=None, kernel_constraint=None,\n",
        "               bias_initializer='zeros', bias_regularizer=None, bias_constraint=None):\n",
        "    super(SLSTMcell, self).__init__()\n",
        "    self.units = units\n",
        "    self.use_bias = use_bias\n",
        "    self.recurrent_function = recurrent_fn\n",
        "    self.activation_fn = activation_fn\n",
        "    self.seq_len = seq_len\n",
        "    self.window = window\n",
        "    self.kernel_initializer = kernel_initializer\n",
        "    self.kernel_regularizer = kernel_regularizer\n",
        "    self.kernel_constraint = kernel_constraint\n",
        "    self.bias_initializer = bias_initializer\n",
        "    self.bias_regularizer = bias_regularizer\n",
        "    self.bias_constraint = bias_constraint\n",
        "    \n",
        "  def build(self, input_shape):\n",
        "    input_dim = input_shape[-1]\n",
        "    \n",
        "    #W\n",
        "    self.W_i = self.add_weight(shape=(self.units * self.window, self.units),\n",
        "                                  name='W_i',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.W_l = self.add_weight(shape=(self.units * self.window, self.units),\n",
        "                                  name='W_l',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.W_r = self.add_weight(shape=(self.units * self.window, self.units),\n",
        "                                  name='W_r',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.W_f = self.add_weight(shape=(self.units * self.window, self.units),\n",
        "                                  name='W_f',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.W_s = self.add_weight(shape=(self.units * self.window, self.units),\n",
        "                                  name='W_s',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.W_o = self.add_weight(shape=(self.units * self.window, self.units),\n",
        "                                  name='W_o',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.W_u = self.add_weight(shape=(self.units * self.window, self.units),\n",
        "                                  name='W_u',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.W_g = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='W_g',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.W_f2 = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='W_f2',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.W_o2 = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='W_o2',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    #U\n",
        "    self.U_i = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_i',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.U_l = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_l',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.U_r = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_r',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.U_f = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_f',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.U_s = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_s',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.U_o = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_o',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.U_u = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_u',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.U_g = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_g',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.U_f2 = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_f2',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.U_o2 = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_o2',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    #V\n",
        "    self.V_i = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='V_i',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.V_l = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='V_l',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.V_r = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='V_r',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.V_f = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='V_f',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.V_s = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='V_s',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.V_o = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='V_o',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.V_u = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='V_u',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "\n",
        "    if self.use_bias:\n",
        "        self.bias_i = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_i',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "        self.bias_l = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_l',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "        self.bias_r = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_r',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "        self.bias_f = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_f',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "        self.bias_s = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_s',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "        self.bias_o = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_o',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "        self.bias_u = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_u',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "        self.bias_g = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_g',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "        self.bias_f2 = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_f2',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "        self.bias_o2 = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_o2',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "    else:\n",
        "        self.bias_i = None\n",
        "        self.bias_l = None\n",
        "        self.bias_r = None\n",
        "        self.bias_f = None\n",
        "        self.bias_s = None\n",
        "        self.bias_o = None\n",
        "        self.bias_u = None\n",
        "        self.bias_g = None\n",
        "        self.bias_f2 = None\n",
        "        self.bias_o2 = None\n",
        "    self.built = True\n",
        "\n",
        "  def call(self, inputs, H_tm1, c_tm1, training=None):\n",
        "\n",
        "    H = H_tm1[0:1, :]\n",
        "    c = c_tm1[0:1, :]\n",
        "\n",
        "    for i in range(1, self.seq_len - 1):\n",
        "      x_i = inputs[:, i-1]\n",
        "      x_l = inputs[:, i-1]\n",
        "      x_r = inputs[:, i-1]\n",
        "      x_f = inputs[:, i-1]\n",
        "      x_s = inputs[:, i-1]\n",
        "      x_o = inputs[:, i-1]\n",
        "      x_u = inputs[:, i-1]\n",
        "      Ux_i = K.dot(x_i, self.U_i)\n",
        "      Ux_l = K.dot(x_l, self.U_l)\n",
        "      Ux_r = K.dot(x_r, self.U_r)\n",
        "      Ux_f = K.dot(x_f, self.U_f)\n",
        "      Ux_s = K.dot(x_s, self.U_s)\n",
        "      Ux_o = K.dot(x_o, self.U_o)\n",
        "      Ux_u = K.dot(x_u, self.U_u)\n",
        "      if self.use_bias:\n",
        "        Ux_i = K.bias_add(Ux_i, self.bias_i)\n",
        "        Ux_l = K.bias_add(Ux_l, self.bias_l)\n",
        "        Ux_r = K.bias_add(Ux_r, self.bias_r)\n",
        "        Ux_f = K.bias_add(Ux_f, self.bias_f)\n",
        "        Ux_s = K.bias_add(Ux_s, self.bias_s)\n",
        "        Ux_o = K.bias_add(Ux_o, self.bias_o)\n",
        "        Ux_u = K.bias_add(Ux_u, self.bias_u)\n",
        "\n",
        "      csi_tm1_i = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "      csi_tm1_l = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "      csi_tm1_r = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "      csi_tm1_f = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "      csi_tm1_s = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "      csi_tm1_o = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "      csi_tm1_u = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "\n",
        "      Wcsi_i = K.dot(csi_tm1_i, self.W_i)\n",
        "      Wcsi_l = K.dot(csi_tm1_l, self.W_l)\n",
        "      Wcsi_r = K.dot(csi_tm1_r, self.W_r)\n",
        "      Wcsi_f = K.dot(csi_tm1_f, self.W_f)\n",
        "      Wcsi_s = K.dot(csi_tm1_s, self.W_s)\n",
        "      Wcsi_o = K.dot(csi_tm1_o, self.W_o)\n",
        "      Wcsi_u = K.dot(csi_tm1_u, self.W_u)\n",
        "      \n",
        "      g_tm1_i = H_tm1[self.seq_len: self.seq_len + 1, :]\n",
        "      g_tm1_l = H_tm1[self.seq_len: self.seq_len + 1, :]\n",
        "      g_tm1_r = H_tm1[self.seq_len: self.seq_len + 1, :]\n",
        "      g_tm1_f = H_tm1[self.seq_len: self.seq_len + 1, :]\n",
        "      g_tm1_s = H_tm1[self.seq_len: self.seq_len + 1, :]\n",
        "      g_tm1_o = H_tm1[self.seq_len: self.seq_len + 1, :]\n",
        "      g_tm1_u = H_tm1[self.seq_len: self.seq_len + 1, :]\n",
        "      Vg_i = K.dot(g_tm1_i, self.V_i)\n",
        "      Vg_l = K.dot(g_tm1_l, self.V_l)\n",
        "      Vg_r = K.dot(g_tm1_r, self.V_r)\n",
        "      Vg_f = K.dot(g_tm1_f, self.V_f)\n",
        "      Vg_s = K.dot(g_tm1_s, self.V_s)\n",
        "      Vg_o = K.dot(g_tm1_o, self.V_o)\n",
        "      Vg_u = K.dot(g_tm1_u, self.V_u)\n",
        "\n",
        "      i_hat = self.recurrent_function(Wcsi_i + Ux_i + Vg_i)\n",
        "      l_hat = self.recurrent_function(Wcsi_l + Ux_l + Vg_l)\n",
        "      r_hat = self.recurrent_function(Wcsi_r + Ux_r + Vg_r)\n",
        "      f_hat = self.recurrent_function(Wcsi_f + Ux_f + Vg_f)\n",
        "      s_hat = self.recurrent_function(Wcsi_s + Ux_s + Vg_s)\n",
        "      o_ = self.recurrent_function(Wcsi_o + Ux_o + Vg_o)\n",
        "      u_ = self.activation_fn(Wcsi_u + Ux_u + Vg_u)\n",
        "      \n",
        "      i_, l_, r_, f_, s_ = [tf.keras.activations.softmax(t, axis=0) for t in [i_hat, l_hat, r_hat, f_hat, s_hat]]\n",
        "\n",
        "      c_ = l_ * c_tm1[i-1:i, :] + f_ * c_tm1[i:i+1, :] + r_ * c_tm1[i+1:i+2, :] + \\\n",
        "                                s_ * c_tm1[self.seq_len:, :] + i_ * u_\n",
        "\n",
        "      h_ = o_ * self.activation_fn(c_)\n",
        "      tf.print(h_.shape)\n",
        "\n",
        "      H = tf.concat((H, h_), axis=0)\n",
        "      c = tf.concat((c, c_), axis=0)                                           \n",
        "    \n",
        "    H = tf.concat((H, H_tm1[self.seq_len - 1 : self.seq_len, :]), axis=0)\n",
        "    c = tf.concat((c, c_tm1[self.seq_len - 1 : self.seq_len, :]), axis=0) \n",
        "\n",
        "    # now the calculation to update g\n",
        "\n",
        "    h_bar = tf.reduce_mean(H_tm1[:-1,:], axis=0, keepdims=True)\n",
        "    g_tm1 = H_tm1[self.seq_len : self.seq_len + 1, :]\n",
        "    c_tm1_g = c_tm1[self.seq_len : self.seq_len + 1, :]\n",
        "          \n",
        "    Wg_g = K.dot(g_tm1, self.W_g)\n",
        "    Wg_f2 = K.dot(g_tm1, self.W_f2)\n",
        "    Wg_o2 = K.dot(g_tm1, self.W_o2)\n",
        "    if self.use_bias:\n",
        "      Wg_g = K.bias_add(Wg_g, self.bias_g)\n",
        "      Wg_f2 = K.bias_add(Wg_f2, self.bias_f2)\n",
        "      Wg_o2 = K.bias_add(Wg_o2, self.bias_o2)\n",
        "    Uh_g = K.dot(h_bar, self.U_g)\n",
        "    # this U is different for each i -> so it's in the for loop\n",
        "    Uh_o2 = K.dot(h_bar, self.U_o2)\n",
        "\n",
        "    f_g = self.recurrent_function(Wg_g + Uh_g)\n",
        "    \n",
        "    Uh_f2 = K.dot(H[0:1, :], self.U_f2)\n",
        "    F_ = tf.keras.activations.softmax(self.recurrent_function(Wg_f2 + Uh_f2), axis=0)\n",
        "    for i in range(1, self.seq_len):\n",
        "      Uh_f2 = K.dot(H_tm1[i:i+1, :], self.U_f2)\n",
        "      f_i = tf.keras.activations.softmax(self.recurrent_function(Wg_f2 + Uh_f2), axis=0)\n",
        "      F_ = tf.concat((F_, f_i), axis=0)\n",
        "    o_t = self.recurrent_function(Wg_o2 + Uh_o2)\n",
        "    \n",
        "    c_g = f_g * c_tm1_g + tf.math.reduce_sum((F_[:self.seq_len, :] * c_tm1[:self.seq_len, :]), axis=0, keepdims=True)\n",
        "    g_t = o_t * self.activation_fn(c_g)\n",
        "\n",
        "    H = tf.concat((H, g_t,), axis=0)\n",
        "    c = tf.concat((c, c_g,), axis=0)\n",
        "\n",
        "    # tf.print(inputs.shape, H.shape, c.shape, H_tm1.shape, c_tm1.shape, g_t.shape)\n",
        "      \n",
        "    return inputs, H, c, g_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQuspG4ZGevP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SLSTMcell(keras.layers.Layer):\n",
        "  def __init__(self, units=50, window=3, use_bias=True, recurrent_fn=sigmoid, activation_fn=tanh, seq_len=max_len, \n",
        "               kernel_initializer='uniform', kernel_regularizer=None, kernel_constraint=None,\n",
        "               bias_initializer='zeros', bias_regularizer=None, bias_constraint=None):\n",
        "    super(SLSTMcell, self).__init__()\n",
        "    self.units = units\n",
        "    self.use_bias = use_bias\n",
        "    self.recurrent_function = recurrent_fn\n",
        "    self.activation_fn = activation_fn\n",
        "    self.seq_len = seq_len\n",
        "    self.window = window\n",
        "    self.kernel_initializer = kernel_initializer\n",
        "    self.kernel_regularizer = kernel_regularizer\n",
        "    self.kernel_constraint = kernel_constraint\n",
        "    self.bias_initializer = bias_initializer\n",
        "    self.bias_regularizer = bias_regularizer\n",
        "    self.bias_constraint = bias_constraint\n",
        "    \n",
        "  def build(self, input_shape):\n",
        "    input_dim = input_shape[-1]\n",
        "    \n",
        "    #W\n",
        "    self.W_i = self.add_weight(shape=(self.units * self.window, self.units),\n",
        "                                  name='W_i',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.W_l = self.add_weight(shape=(self.units * self.window, self.units),\n",
        "                                  name='W_l',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.W_r = self.add_weight(shape=(self.units * self.window, self.units),\n",
        "                                  name='W_r',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.W_f = self.add_weight(shape=(self.units * self.window, self.units),\n",
        "                                  name='W_f',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.W_s = self.add_weight(shape=(self.units * self.window, self.units),\n",
        "                                  name='W_s',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.W_o = self.add_weight(shape=(self.units * self.window, self.units),\n",
        "                                  name='W_o',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.W_u = self.add_weight(shape=(self.units * self.window, self.units),\n",
        "                                  name='W_u',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.W_g = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='W_g',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.W_f2 = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='W_f2',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.W_o2 = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='W_o2',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    #U\n",
        "    self.U_i = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_i',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.U_l = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_l',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.U_r = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_r',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.U_f = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_f',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.U_s = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_s',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.U_o = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_o',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.U_u = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_u',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.U_g = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_g',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.U_f2 = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_f2',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.U_o2 = self.add_weight(shape=(input_dim, self.units),\n",
        "                                  name='U_o2',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    #V\n",
        "    self.V_i = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='V_i',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.V_l = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='V_l',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.V_r = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='V_r',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.V_f = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='V_f',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.V_s = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='V_s',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.V_o = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='V_o',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "    self.V_u = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='V_u',\n",
        "                                  initializer=self.kernel_initializer)\n",
        "\n",
        "    if self.use_bias:\n",
        "        self.bias_i = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_i',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "        self.bias_l = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_l',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "        self.bias_r = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_r',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "        self.bias_f = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_f',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "        self.bias_s = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_s',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "        self.bias_o = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_o',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "        self.bias_u = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_u',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "        self.bias_g = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_g',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "        self.bias_f2 = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_f2',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "        self.bias_o2 = self.add_weight(shape=(self.units,),\n",
        "                                    name='bias_o2',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "    else:\n",
        "        self.bias_i = None\n",
        "        self.bias_l = None\n",
        "        self.bias_r = None\n",
        "        self.bias_f = None\n",
        "        self.bias_s = None\n",
        "        self.bias_o = None\n",
        "        self.bias_u = None\n",
        "        self.bias_g = None\n",
        "        self.bias_f2 = None\n",
        "        self.bias_o2 = None\n",
        "    self.built = True\n",
        "\n",
        "  def call(self, inputs, H_tm1, c_tm1, training=None):\n",
        "\n",
        "    H = H_tm1[0:1, :]\n",
        "    c = c_tm1[0:1, :]\n",
        "\n",
        "    for i in range(1, self.seq_len - 1):\n",
        "      x_i = inputs[:, i]\n",
        "      x_l = inputs[:, i]\n",
        "      x_r = inputs[:, i]\n",
        "      x_f = inputs[:, i]\n",
        "      x_s = inputs[:, i]\n",
        "      x_o = inputs[:, i]\n",
        "      x_u = inputs[:, i]\n",
        "      Ux_i = K.dot(x_i, self.U_i)\n",
        "      Ux_l = K.dot(x_l, self.U_l)\n",
        "      Ux_r = K.dot(x_r, self.U_r)\n",
        "      Ux_f = K.dot(x_f, self.U_f)\n",
        "      Ux_s = K.dot(x_s, self.U_s)\n",
        "      Ux_o = K.dot(x_o, self.U_o)\n",
        "      Ux_u = K.dot(x_u, self.U_u)\n",
        "      if self.use_bias:\n",
        "        Ux_i = K.bias_add(Ux_i, self.bias_i)\n",
        "        Ux_l = K.bias_add(Ux_l, self.bias_l)\n",
        "        Ux_r = K.bias_add(Ux_r, self.bias_r)\n",
        "        Ux_f = K.bias_add(Ux_f, self.bias_f)\n",
        "        Ux_s = K.bias_add(Ux_s, self.bias_s)\n",
        "        Ux_o = K.bias_add(Ux_o, self.bias_o)\n",
        "        Ux_u = K.bias_add(Ux_u, self.bias_u)\n",
        "\n",
        "      csi_tm1_i = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "      csi_tm1_l = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "      csi_tm1_r = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "      csi_tm1_f = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "      csi_tm1_s = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "      csi_tm1_o = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "      csi_tm1_u = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "\n",
        "      Wcsi_i = K.dot(csi_tm1_i, self.W_i)\n",
        "      Wcsi_l = K.dot(csi_tm1_l, self.W_l)\n",
        "      Wcsi_r = K.dot(csi_tm1_r, self.W_r)\n",
        "      Wcsi_f = K.dot(csi_tm1_f, self.W_f)\n",
        "      Wcsi_s = K.dot(csi_tm1_s, self.W_s)\n",
        "      Wcsi_o = K.dot(csi_tm1_o, self.W_o)\n",
        "      Wcsi_u = K.dot(csi_tm1_u, self.W_u)\n",
        "      \n",
        "      g_tm1_i = H_tm1[self.seq_len: self.seq_len + 1, :]\n",
        "      g_tm1_l = H_tm1[self.seq_len: self.seq_len + 1, :]\n",
        "      g_tm1_r = H_tm1[self.seq_len: self.seq_len + 1, :]\n",
        "      g_tm1_f = H_tm1[self.seq_len: self.seq_len + 1, :]\n",
        "      g_tm1_s = H_tm1[self.seq_len: self.seq_len + 1, :]\n",
        "      g_tm1_o = H_tm1[self.seq_len: self.seq_len + 1, :]\n",
        "      g_tm1_u = H_tm1[self.seq_len: self.seq_len + 1, :]\n",
        "      Vg_i = K.dot(g_tm1_i, self.V_i)\n",
        "      Vg_l = K.dot(g_tm1_l, self.V_l)\n",
        "      Vg_r = K.dot(g_tm1_r, self.V_r)\n",
        "      Vg_f = K.dot(g_tm1_f, self.V_f)\n",
        "      Vg_s = K.dot(g_tm1_s, self.V_s)\n",
        "      Vg_o = K.dot(g_tm1_o, self.V_o)\n",
        "      Vg_u = K.dot(g_tm1_u, self.V_u)\n",
        "\n",
        "      i_hat = self.recurrent_function(Wcsi_i + Ux_i + Vg_i)\n",
        "      l_hat = self.recurrent_function(Wcsi_l + Ux_l + Vg_l)\n",
        "      r_hat = self.recurrent_function(Wcsi_r + Ux_r + Vg_r)\n",
        "      f_hat = self.recurrent_function(Wcsi_f + Ux_f + Vg_f)\n",
        "      s_hat = self.recurrent_function(Wcsi_s + Ux_s + Vg_s)\n",
        "      o_ = self.recurrent_function(Wcsi_o + Ux_o + Vg_o)\n",
        "      u_ = self.activation_fn(Wcsi_u + Ux_u + Vg_u)\n",
        "      \n",
        "      i_, l_, r_, f_, s_ = [tf.keras.activations.softmax(t, axis=0) for t in [i_hat, l_hat, r_hat, f_hat, s_hat]]\n",
        "\n",
        "      c_ = l_ * c_tm1[i-1:i, :] + f_ * c_tm1[i:i+1, :] + r_ * c_tm1[i+1:i+2, :] + \\\n",
        "                                s_ * c_tm1[self.seq_len:self.seq_len+1, :] + i_ * u_\n",
        "\n",
        "      h_ = o_ * self.activation_fn(c_)\n",
        "\n",
        "      H = tf.concat((H, h_), axis=0)\n",
        "      c = tf.concat((c, c_), axis=0)                                           \n",
        "    \n",
        "    H = tf.concat((H, H_tm1[self.seq_len - 1 : self.seq_len, :]), axis=0)\n",
        "    c = tf.concat((c, c_tm1[self.seq_len - 1 : self.seq_len, :]), axis=0) \n",
        "\n",
        "    # now the calculation to update g\n",
        "\n",
        "    h_bar = tf.reduce_mean(H_tm1[:-1,:], axis=0, keepdims=True)\n",
        "    g_tm1 = H_tm1[self.seq_len : self.seq_len + 1, :]\n",
        "    c_tm1_g = c_tm1[self.seq_len : self.seq_len + 1, :]\n",
        "          \n",
        "    Wg_g = K.dot(g_tm1, self.W_g)\n",
        "    Wg_f2 = K.dot(g_tm1, self.W_f2)\n",
        "    Wg_o2 = K.dot(g_tm1, self.W_o2)\n",
        "    if self.use_bias:\n",
        "      Wg_g = K.bias_add(Wg_g, self.bias_g)\n",
        "      Wg_f2 = K.bias_add(Wg_f2, self.bias_f2)\n",
        "      Wg_o2 = K.bias_add(Wg_o2, self.bias_o2)\n",
        "    Uh_g = K.dot(h_bar, self.U_g)\n",
        "    # this U is different for each i -> so it's in the for loop\n",
        "    Uh_o2 = K.dot(h_bar, self.U_o2)\n",
        "\n",
        "    f_g = self.recurrent_function(Wg_g + Uh_g)\n",
        "    \n",
        "    Uh_f2 = K.dot(H[0:1, :], self.U_f2)\n",
        "    F_ = tf.keras.activations.softmax(self.recurrent_function(Wg_f2 + Uh_f2), axis=0)\n",
        "    for i in range(1, self.seq_len):\n",
        "      Uh_f2 = K.dot(H_tm1[i:i+1, :], self.U_f2)\n",
        "      f_i = tf.keras.activations.softmax(self.recurrent_function(Wg_f2 + Uh_f2), axis=0)\n",
        "      F_ = tf.concat((F_, f_i), axis=0)\n",
        "    o_t = self.recurrent_function(Wg_o2 + Uh_o2)\n",
        "    \n",
        "    c_g = f_g * c_tm1_g + tf.math.reduce_sum((F_[:self.seq_len, :] * c_tm1[:self.seq_len, :]), axis=0, keepdims=True)\n",
        "    g_t = o_t * self.activation_fn(c_g)\n",
        "\n",
        "    H = tf.concat((H, g_t,), axis=0)\n",
        "    c = tf.concat((c, c_g,), axis=0)\n",
        "\n",
        "    # tf.print(inputs.shape, H.shape, c.shape, H_tm1.shape, c_tm1.shape, g_t.shape)\n",
        "      \n",
        "    return inputs, H, c, g_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Bm_ctimpNY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SLSTM(keras.layers.Layer):\n",
        "  def __init__(self, cell, n_cells,\n",
        "               kernel_initializer='uniform', kernel_regularizer=None, kernel_constraint=None):\n",
        "    super(SLSTM, self).__init__()\n",
        "    self.cell = cell\n",
        "    self.n_cells = n_cells\n",
        "    self.units = self.cell.units\n",
        "    self.kernel_initializer = kernel_initializer\n",
        "    self.kernel_regularizer = kernel_regularizer\n",
        "    self.kernel_constraint = kernel_constraint\n",
        "\n",
        "  def build(self, input_shape, seq_len=100):\n",
        "    if not seq_len:\n",
        "      seq_len = input_shape[-2]\n",
        "    self.H = K.variable(value = tf.random.uniform(shape=(seq_len + 1, self.units), minval=-0.05, maxval=0.05, dtype=tf.float32))\n",
        "    self.c = K.variable(value = tf.random.uniform(shape=(seq_len + 1, self.units), minval=-0.05, maxval=0.05, dtype=tf.float32))\n",
        "    # self.H = tf.random.uniform(shape=(seq_len + 1, self.units), minval=-0.05, maxval=0.05, dtype=tf.float32)\n",
        "    # self.c = tf.random.uniform(shape=(seq_len + 1, self.units), minval=-0.05, maxval=0.05, dtype=tf.float32)\n",
        "    \n",
        "  def call(self, inputs, training=None):\n",
        "    H, c = self.H, self.c\n",
        "    for _ in range(self.n_cells):\n",
        "      inputs, H, c, g_ = self.cell(inputs, H, c)\n",
        "    return g_\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKMFLM7faVQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "units = 50\n",
        "model = Sequential([Embedding(num_tokens, embedding_dim, embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable=True,),\n",
        "                    SLSTM(SLSTMcell(units=units), 2),\n",
        "                    keras.layers.Dense(1, activation='sigmoid')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmvAPperGkDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = losses.BinaryCrossentropy()\n",
        "optimizer = optimizers.Adam()\n",
        "acc = metrics.BinaryAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPHa9XVOFX3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=loss, optimizer=optimizer, metrics=[acc])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og5CoCB4Fjet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(train_data.shuffle(1000).batch(32), epochs=20, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2URLx5lrOWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
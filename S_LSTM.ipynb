{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XCZgG1WAsqysR3bGoB1pshvvocir9P6Z",
      "authorship_tag": "ABX9TyPtCTcbpDpPlp+Rr1fZ4CMY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabboM/NNDS/blob/master/S_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um7oKpoM5PwK",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks for Data Science Applications\n",
        "\n",
        "Code and work is related to this [Paper](https://arxiv.org/pdf/1805.02474.pdf)\n",
        "and some code is adapted from [here](https://keras.io/examples/nlp/pretrained_word_embeddings/) and [here](https://medium.com/softmax/tensorflow-keras-lstm-source-code-line-by-line-explained-125a6dae0622)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soQVjZmolB8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJeBavA453kc",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mff73X42QyUh",
        "colab_type": "text"
      },
      "source": [
        "### Loading IMDB_reviews and splitting in Train/Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPFM_ExG5b2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews/plain_text',\n",
        "                                          split=['train', 'test'],\n",
        "                                          shuffle_files=True,\n",
        "                                          as_supervised=True,\n",
        "                                          with_info=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgBAsKeLRKX8",
        "colab_type": "text"
      },
      "source": [
        "creating a list `ds` of all the reviews in plain text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlxnM6v28NVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "it = list(ds_train)\n",
        "ds = []\n",
        "for i in it:\n",
        "  ds.append(i[0].numpy().decode())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHeNNRoXgkAp",
        "colab_type": "text"
      },
      "source": [
        "Let's download the GloVe word embeddings. We will use dim=100 instead of 300 to speed up the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKlCN0BJYUW4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "cf89eec1-0433-4a31-c873-b8b3aae589c4"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-05 12:49:50--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-09-05 12:49:50--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-09-05 12:49:50--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.2’\n",
            "\n",
            "glove.6B.zip.2      100%[===================>] 822.24M  2.18MB/s    in 6m 27s  \n",
            "\n",
            "2020-09-05 12:56:17 (2.13 MB/s) - ‘glove.6B.zip.2’ saved [862182613/862182613]\n",
            "\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBwB_-ixkzhj",
        "colab_type": "text"
      },
      "source": [
        "creating a mapping of the words to their vector representation by GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NVdXHklkwiq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "62a9cc77-80e0-4f01-dcf7-b3814c9eff50"
      },
      "source": [
        "path_to_glove_file = os.path.join(\n",
        "    \"glove.6B.100d.txt\"\n",
        ")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFtRkRTOSBeb",
        "colab_type": "text"
      },
      "source": [
        "Now we should have a tokenizer for the raw text. We train a word tokenizer on the training corpus and create a dictionary with the corresponding vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0E1mmFxzq83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_dataset = tf.data.Dataset.from_tensor_slices(ds)\n",
        "max_features = 20000  # Maximum vocab size.\n",
        "max_len = 200  # Sequence length to pad the outputs to.\n",
        "\n",
        "# Create the layer.\n",
        "vectorize = TextVectorization(\n",
        " max_tokens=max_features,\n",
        " output_mode='int',\n",
        " output_sequence_length=max_len)\n",
        "\n",
        "# Now that the vocab layer has been created, call `adapt` on the text-only\n",
        "# dataset to create the vocabulary. You don't have to batch, but for large\n",
        "# datasets this means we're not keeping spare copies of the dataset.\n",
        "vectorize.adapt(text_dataset.batch(64))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKx0HkfC9m-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voc = vectorize.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS5m-YzFSoV_",
        "colab_type": "text"
      },
      "source": [
        "creating an embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CouWkFjo9Hq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b0248ab6-34b3-479e-a425-65c20485683d"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converted 18723 words (1277 misses)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVPwSnBA9yaC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "88a8703f-f02a-4918-d6ba-afb8a3dd8c1b"
      },
      "source": [
        "len(embeddings_index.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMAaLEbXTag_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "26901f0e-2775-4c31-93a8-ab4f76f21317"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20002, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCQHcJM_cR3A",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CBmhZpe7l0q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a610b359-145f-439a-aa3a-58c338c15a40"
      },
      "source": [
        "class SLSTMcell(keras.layers.Layer):\n",
        "    def __init__(self, units=32):\n",
        "        super(SLTMcell, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.Wi = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.Ui = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.Vi = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.bi = self.add_weight(\n",
        "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kREjOqegd6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras import backend as K"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM1im9tHmgOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.constant([1,2,3], shape=(3,1))\n",
        "W = tf.constant([[1,1,1],[2,2,2]])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j99xXQ4Omm0V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5e9d7c65-38c7-4796-97e6-d3b4b133f13b"
      },
      "source": [
        "K.dot(W,x)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n",
              "array([[ 6],\n",
              "       [12]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxREQ6p7moeT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2ef31591-2077-49f8-bf7a-bcbda31ab418"
      },
      "source": [
        "K.int_shape(W)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEbYXokepGrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build(self, input_shape):\n",
        "  input_dim = input_shape[-1]\n",
        "\n",
        "  self.U = self.add_weight(shape=(input_dim, self.units * 4),\n",
        "                                name='kernel',\n",
        "                                initializer=self.kernel_initializer,\n",
        "                                regularizer=self.kernel_regularizer,\n",
        "                                constraint=self.kernel_constraint)\n",
        "  self.recurrent_kernel = self.add_weight(\n",
        "      shape=(self.units, self.units * 4),\n",
        "      name='recurrent_kernel',\n",
        "      initializer=self.recurrent_initializer,\n",
        "      regularizer=self.recurrent_regularizer,\n",
        "      constraint=self.recurrent_constraint)\n",
        "\n",
        "  if self.use_bias:\n",
        "      if self.unit_forget_bias:\n",
        "          @K.eager\n",
        "          def bias_initializer(_, *args, **kwargs):\n",
        "              return K.concatenate([\n",
        "                  self.bias_initializer((self.units,), *args, **kwargs),\n",
        "                  initializers.Ones()((self.units,), *args, **kwargs),\n",
        "                  self.bias_initializer((self.units * 2,), *args, **kwargs),\n",
        "              ])\n",
        "      else:\n",
        "          bias_initializer = self.bias_initializer\n",
        "      self.bias = self.add_weight(shape=(self.units * 4,),\n",
        "                                  name='bias',\n",
        "                                  initializer=bias_initializer,\n",
        "                                  regularizer=self.bias_regularizer,\n",
        "                                  constraint=self.bias_constraint)\n",
        "  else:\n",
        "      self.bias = None\n",
        "\n",
        "  self.kernel_i = self.kernel[:, :self.units]\n",
        "  self.kernel_f = self.kernel[:, self.units: self.units * 2]\n",
        "  self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n",
        "  self.kernel_o = self.kernel[:, self.units * 3:]\n",
        "\n",
        "  self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n",
        "  self.recurrent_kernel_f = (\n",
        "      self.recurrent_kernel[:, self.units: self.units * 2])\n",
        "  self.recurrent_kernel_c = (\n",
        "      self.recurrent_kernel[:, self.units * 2: self.units * 3])\n",
        "  self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n",
        "\n",
        "  if self.use_bias:\n",
        "      self.bias_i = self.bias[:self.units]\n",
        "      self.bias_f = self.bias[self.units: self.units * 2]\n",
        "      self.bias_c = self.bias[self.units * 2: self.units * 3]\n",
        "      self.bias_o = self.bias[self.units * 3:]\n",
        "  else:\n",
        "      self.bias_i = None\n",
        "      self.bias_f = None\n",
        "      self.bias_c = None\n",
        "      self.bias_o = None\n",
        "  self.built = True"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
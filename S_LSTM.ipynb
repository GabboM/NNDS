{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XCZgG1WAsqysR3bGoB1pshvvocir9P6Z",
      "authorship_tag": "ABX9TyNwDIcgoIEYEw/s65lFJX+p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabboM/NNDS/blob/master/S_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um7oKpoM5PwK",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks for Data Science Applications\n",
        "\n",
        "Code and work is related to this [Paper](https://arxiv.org/pdf/1805.02474.pdf)\n",
        "and some code is adapted from [here](https://keras.io/examples/nlp/pretrained_word_embeddings/) and [here](https://medium.com/softmax/tensorflow-keras-lstm-source-code-line-by-line-explained-125a6dae0622)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soQVjZmolB8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.keras.activations import sigmoid, tanh\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJeBavA453kc",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mff73X42QyUh",
        "colab_type": "text"
      },
      "source": [
        "### Loading IMDB_reviews and splitting in Train/Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPFM_ExG5b2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews/plain_text',\n",
        "                                          split=['train', 'test'],\n",
        "                                          shuffle_files=True,\n",
        "                                          as_supervised=True,\n",
        "                                          with_info=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgBAsKeLRKX8",
        "colab_type": "text"
      },
      "source": [
        "creating a list `ds` of all the reviews in plain text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlxnM6v28NVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "it = list(ds_train)\n",
        "ds = []\n",
        "for _ in range(10000): #manual tokens\n",
        "  ds.append('startofsentence endofsentence')\n",
        "for i in it:\n",
        "  ds.append(i[0].numpy().decode())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHeNNRoXgkAp",
        "colab_type": "text"
      },
      "source": [
        "Let's download the GloVe word embeddings. We will use dim=100 instead of 300 to speed up the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKlCN0BJYUW4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "642898ba-0439-4804-b4a8-2cc1a5d12caa"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-08 08:51:27--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-09-08 08:51:28--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-09-08 08:51:28--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1        6%[>                   ]  53.79M  12.7MB/s    eta 47s    ^C\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "N\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBwB_-ixkzhj",
        "colab_type": "text"
      },
      "source": [
        "creating a mapping of the words to their vector representation by GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NVdXHklkwiq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "842e426b-652f-4ba5-d69a-e059ed8f9572"
      },
      "source": [
        "path_to_glove_file = os.path.join(\n",
        "    \"glove.6B.100d.txt\"\n",
        ")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFtRkRTOSBeb",
        "colab_type": "text"
      },
      "source": [
        "Now we should have a tokenizer for the raw text. We train a word tokenizer on the training corpus and create a dictionary with the corresponding vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0E1mmFxzq83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_dataset = tf.data.Dataset.from_tensor_slices(ds)\n",
        "max_features = 20000  # Maximum vocab size.\n",
        "max_len = 200  # Sequence length to pad the outputs to.\n",
        "\n",
        "# Create the layer.\n",
        "vectorize = TextVectorization(\n",
        " max_tokens=max_features,\n",
        " output_mode='int',\n",
        " output_sequence_length=max_len)\n",
        "\n",
        "# Now that the vocab layer has been created, call `adapt` on the text-only\n",
        "# dataset to create the vocabulary. You don't have to batch, but for large\n",
        "# datasets this means we're not keeping spare copies of the dataset.\n",
        "vectorize.adapt(text_dataset.batch(64))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKx0HkfC9m-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voc = vectorize.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6TJt1Lw7Hw9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "efc7e04c-520c-4f1e-ccbd-96c50b28b44b"
      },
      "source": [
        "print('startofsentence' in voc)\n",
        "print('endofsentence' in voc)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5nBxHNL1TS0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c09cf40d-902c-46a3-8a61-ef9d3b82787a"
      },
      "source": [
        "voc[2]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS5m-YzFSoV_",
        "colab_type": "text"
      },
      "source": [
        "creating an embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CouWkFjo9Hq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3dc000d-a57b-4eb8-fd6a-9fefadbb6192"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converted 18721 words (1279 misses)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVPwSnBA9yaC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "66f96b1a-48cf-4585-91a0-5362c5a24a96"
      },
      "source": [
        "len(embeddings_index.keys())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMAaLEbXTag_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e09543a-c3fb-4a0a-aac0-28572d9f5a26"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20002, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRkwqaKO1MqO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "e4c389c2-02d4-4422-ee4c-31c2ee18814f"
      },
      "source": [
        "embedding_matrix[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCQHcJM_cR3A",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAmh_RCv2qzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=True,\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6y3_oUv7zkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(x):\n",
        "  return('startofsentence ' + x + ' endofsentence')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt3J_iOCBkOS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "78423930-0a35-4651-d4b7-baf54e2bd719"
      },
      "source": [
        "sent = preprocess_sentence(\"the cat sat on the mat\")\n",
        "output = vectorize([sent])\n",
        "output"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 200), dtype=int64, numpy=\n",
              "array([[   71,     2,  1149,  1753,    21,     2, 12530,    72,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JCOCKfdBp_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = embedding_layer(output)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CBmhZpe7l0q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "945545fb-d261-4494-ba25-c8c7f9085eb5"
      },
      "source": [
        "class SLSTMcell(keras.layers.Layer):\n",
        "  def __init__(self, units=100, window=3, use_bias=True, recurrent_fn=sigmoid, activation_fn=tanh, seq_len=max_len, \n",
        "               kernel_initializer='uniform', kernel_regularizer=None, kernel_constraint=None,\n",
        "               bias_initializer='zeros', bias_regularizer=None, bias_constraint=None):\n",
        "    super(SLSTMcell, self).__init__()\n",
        "    self.units = units\n",
        "    self.use_bias = use_bias\n",
        "    self.recurrent_function = recurrent_fn\n",
        "    self.activation_fn = activation_fn\n",
        "    self.seq_len = seq_len\n",
        "    self.window = window\n",
        "    self.kernel_initializer = kernel_initializer\n",
        "    self.kernel_regularizer = kernel_regularizer\n",
        "    self.kernel_constraint = kernel_constraint\n",
        "    self.bias_initializer = bias_initializer\n",
        "    self.bias_regularizer = bias_regularizer\n",
        "    self.bias_constraint = bias_constraint\n",
        "    \n",
        "  def build(self, input_shape):\n",
        "    input_dim = input_shape[-1]\n",
        "    self.W = self.add_weight(shape=(input_dim * self.window, self.units * 10),\n",
        "                                  name='W',\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  regularizer=self.kernel_regularizer,\n",
        "                                  constraint=self.kernel_constraint)\n",
        "    \n",
        "    self.W2 = self.add_weight(shape=(input_dim, self.units * 3),\n",
        "                                  name='W2',\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  regularizer=self.kernel_regularizer,\n",
        "                                  constraint=self.kernel_constraint)\n",
        "    \n",
        "    self.U = self.add_weight(shape=(input_dim, self.units * 10),\n",
        "                                  name='U',\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  regularizer=self.kernel_regularizer,\n",
        "                                  constraint=self.kernel_constraint)\n",
        "    \n",
        "    self.V = self.add_weight(shape=(input_dim, self.units * 7),\n",
        "                                  name='V',\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  regularizer=self.kernel_regularizer,\n",
        "                                  constraint=self.kernel_constraint)\n",
        "\n",
        "    if self.use_bias:\n",
        "        # bias_initializer = self.bias_initializer\n",
        "        self.bias = self.add_weight(shape=(self.units * 10,),\n",
        "                                    name='bias',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "    else:\n",
        "        self.bias = None\n",
        "\n",
        "    self.W_i = self.W[:, :self.units]\n",
        "    self.W_l = self.W[:, self.units * 1: self.units * 2]\n",
        "    self.W_r = self.W[:, self.units * 2: self.units * 3]\n",
        "    self.W_f = self.W[:, self.units * 3: self.units * 4]\n",
        "    self.W_s = self.W[:, self.units * 4: self.units * 5]\n",
        "    self.W_o = self.W[:, self.units * 5: self.units * 6]\n",
        "    self.W_u = self.W[:, self.units * 6: self.units * 7]\n",
        "    self.W_g = self.W2[:, :self.units]\n",
        "    self.W_f2 = self.W2[:, self.units * 1: self.units * 2]\n",
        "    self.W_o2 = self.W2[:, self.units * 2:]\n",
        "\n",
        "    self.U_i = self.U[:, :self.units]\n",
        "    self.U_l = self.U[:, self.units * 1: self.units * 2]\n",
        "    self.U_r = self.U[:, self.units * 2: self.units * 3]\n",
        "    self.U_f = self.U[:, self.units * 3: self.units * 4]\n",
        "    self.U_s = self.U[:, self.units * 4: self.units * 5]\n",
        "    self.U_o = self.U[:, self.units * 5: self.units * 6]\n",
        "    self.U_u = self.U[:, self.units * 6: self.units * 7]\n",
        "    self.U_g = self.U[:, self.units * 7: self.units * 8]\n",
        "    self.U_f2 = self.U[:, self.units * 8: self.units * 9]\n",
        "    self.U_o2 = self.U[:, self.units * 9:]\n",
        "    \n",
        "    self.V_i = self.V[:, :self.units]\n",
        "    self.V_l = self.V[:, self.units * 1: self.units * 2]\n",
        "    self.V_r = self.V[:, self.units * 2: self.units * 3]\n",
        "    self.V_f = self.V[:, self.units * 3: self.units * 4]\n",
        "    self.V_s = self.V[:, self.units * 4: self.units * 5]\n",
        "    self.V_o = self.V[:, self.units * 5: self.units * 6]\n",
        "    self.V_u = self.V[:, self.units * 6:]\n",
        "\n",
        "    if self.use_bias:\n",
        "        self.bias_i = self.bias[:self.units]\n",
        "        self.bias_l = self.bias[self.units * 1: self.units * 2]\n",
        "        self.bias_r = self.bias[self.units * 2: self.units * 3]\n",
        "        self.bias_f = self.bias[self.units * 3: self.units * 4]\n",
        "        self.bias_s = self.bias[self.units * 4: self.units * 5]\n",
        "        self.bias_o = self.bias[self.units * 5: self.units * 6]\n",
        "        self.bias_u = self.bias[self.units * 6: self.units * 7]\n",
        "        self.bias_g = self.bias[self.units * 7: self.units * 8]\n",
        "        self.bias_f2 = self.bias[self.units * 8: self.units * 9]\n",
        "        self.bias_o2 = self.bias[self.units * 9:]\n",
        "    else:\n",
        "        self.bias_i = None\n",
        "        self.bias_l = None\n",
        "        self.bias_r = None\n",
        "        self.bias_f = None\n",
        "        self.bias_s = None\n",
        "        self.bias_o = None\n",
        "        self.bias_u = None\n",
        "        self.bias_g = None\n",
        "        self.bias_f2 = None\n",
        "        self.bias_o2 = None\n",
        "    self.built = True\n",
        "\n",
        "  def call(self, inputs, states, training=None):\n",
        "\n",
        "    # H_tm1 = states[:, :self.seq_len + 2]  # previous memory state\n",
        "    # c_tm1 = states[:, self.seq_len + 2:]  # previous carry state\n",
        "\n",
        "    H_tm1 = states[0]\n",
        "    c_tm1 = states[1]\n",
        "    H = tf.identity(H_tm1)\n",
        "    c = tf.identity(c_tm1)\n",
        "\n",
        "    for i in range(1, self.seq_len):\n",
        "      x_i = inputs[:, i-1]\n",
        "      x_l = inputs[:, i-1]\n",
        "      x_r = inputs[:, i-1]\n",
        "      x_f = inputs[:, i-1]\n",
        "      x_s = inputs[:, i-1]\n",
        "      x_o = inputs[:, i-1]\n",
        "      x_u = inputs[:, i-1]\n",
        "      Ux_i = K.dot(x_i, self.U_i)\n",
        "      Ux_l = K.dot(x_l, self.U_l)\n",
        "      Ux_r = K.dot(x_r, self.U_r)\n",
        "      Ux_f = K.dot(x_f, self.U_f)\n",
        "      Ux_s = K.dot(x_s, self.U_s)\n",
        "      Ux_o = K.dot(x_o, self.U_o)\n",
        "      Ux_u = K.dot(x_u, self.U_u)\n",
        "      if self.use_bias:\n",
        "        Ux_i = K.bias_add(Ux_i, self.bias_i)\n",
        "        Ux_l = K.bias_add(Ux_l, self.bias_l)\n",
        "        Ux_r = K.bias_add(Ux_r, self.bias_r)\n",
        "        Ux_f = K.bias_add(Ux_f, self.bias_f)\n",
        "        Ux_s = K.bias_add(Ux_s, self.bias_s)\n",
        "        Ux_o = K.bias_add(Ux_o, self.bias_o)\n",
        "        Ux_u = K.bias_add(Ux_u, self.bias_u)\n",
        "\n",
        "      csi_tm1_i = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "      csi_tm1_l = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "      csi_tm1_r = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "      csi_tm1_f = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "      csi_tm1_s = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "      csi_tm1_o = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "      csi_tm1_u = tf.concat((H_tm1[i-1:i, :], H_tm1[i:i+1, :], H_tm1[i+1:i+2, :]), axis=1)\n",
        "\n",
        "      Wcsi_i = K.dot(csi_tm1_i, self.W_i)\n",
        "      Wcsi_l = K.dot(csi_tm1_l, self.W_l)\n",
        "      Wcsi_r = K.dot(csi_tm1_r, self.W_r)\n",
        "      Wcsi_f = K.dot(csi_tm1_f, self.W_f)\n",
        "      Wcsi_s = K.dot(csi_tm1_s, self.W_s)\n",
        "      Wcsi_o = K.dot(csi_tm1_o, self.W_o)\n",
        "      Wcsi_u = K.dot(csi_tm1_u, self.W_u)\n",
        "      \n",
        "      g_tm1_i = H_tm1[self.seq_len + 1:, :]\n",
        "      g_tm1_l = H_tm1[self.seq_len + 1:, :]\n",
        "      g_tm1_r = H_tm1[self.seq_len + 1:, :]\n",
        "      g_tm1_f = H_tm1[self.seq_len + 1:, :]\n",
        "      g_tm1_s = H_tm1[self.seq_len + 1:, :]\n",
        "      g_tm1_o = H_tm1[self.seq_len + 1:, :]\n",
        "      g_tm1_u = H_tm1[self.seq_len + 1:, :]\n",
        "      Vg_i = K.dot(g_tm1_i, self.V_i)\n",
        "      Vg_l = K.dot(g_tm1_l, self.V_l)\n",
        "      Vg_r = K.dot(g_tm1_r, self.V_r)\n",
        "      Vg_f = K.dot(g_tm1_f, self.V_f)\n",
        "      Vg_s = K.dot(g_tm1_s, self.V_s)\n",
        "      Vg_o = K.dot(g_tm1_o, self.V_o)\n",
        "      Vg_u = K.dot(g_tm1_u, self.V_u)\n",
        "\n",
        "      i_hat = self.recurrent_function(Wcsi_i + Ux_i + Vg_i)\n",
        "      l_hat = self.recurrent_function(Wcsi_l + Ux_l + Vg_l)\n",
        "      r_hat = self.recurrent_function(Wcsi_r + Ux_r + Vg_r)\n",
        "      f_hat = self.recurrent_function(Wcsi_f + Ux_f + Vg_f)\n",
        "      s_hat = self.recurrent_function(Wcsi_s + Ux_s + Vg_s)\n",
        "      o_ = self.recurrent_function(Wcsi_o + Ux_o + Vg_o)\n",
        "      u_ = self.activation_fn(Wcsi_u + Ux_u + Vg_u)\n",
        "      \n",
        "      i_, l_, r_, f_, s_ = [tf.keras.activations.softmax(t, axis=0) for t in [i_hat, l_hat, r_hat, f_hat, s_hat]]\n",
        "      \n",
        "      c_ = l_ * c_tm1[i-1:i, :] + f_ * c_tm1[i:i+1, :] + r_ * c_tm1[i+1:i+2, :] + \\\n",
        "                                s_ * c_tm1[self.seq_len + 1:, :] + i_ * u_\n",
        "\n",
        "      h_ = o_ * self.activation_fn(c_)\n",
        "    \n",
        "      H = tf.concat((H[:i, :], h_, H[i+1:, :]), axis=0)\n",
        "      c = tf.concat((c[:i, :], c_, c[i+1:, :]), axis=0)                                           \n",
        "    \n",
        "      # now the calculation to update g\n",
        "    h_bar = tf.reduce_mean(H[:-1,:], axis=0, keepdims=True)\n",
        "    g_tm1 = H[self.seq_len + 1:, :]\n",
        "    c_tm1_g = c_tm1[self.seq_len + 1:, :]\n",
        "          \n",
        "    Wg_g = K.dot(g_tm1, self.W_g)\n",
        "    Wg_f2 = K.dot(g_tm1, self.W_f2)\n",
        "    Wg_o2 = K.dot(g_tm1, self.W_o2)\n",
        "    if self.use_bias:\n",
        "      Wg_g = K.bias_add(Wg_g, self.bias_g)\n",
        "      Wg_f2 = K.bias_add(Wg_f2, self.bias_f2)\n",
        "      Wg_o2 = K.bias_add(Wg_o2, self.bias_o2)\n",
        "    Uh_g = K.dot(h_bar, self.U_g)\n",
        "    # this U is different for each i -> so it's in the for loop\n",
        "    Uh_o2 = K.dot(h_bar, self.U_o2)\n",
        "\n",
        "    f_g = self.recurrent_function(Wg_g + Uh_g)\n",
        "    \n",
        "    Uh_f2 = K.dot(H[0:1, :], self.U_f2)\n",
        "    F_ = tf.keras.activations.softmax(self.recurrent_function(Wg_f2 + Uh_f2), axis=0)\n",
        "    for i in range(1, self.seq_len + 2):\n",
        "      Uh_f2 = K.dot(H[i:i+1, :], self.U_f2)\n",
        "      f_i = tf.keras.activations.softmax(self.recurrent_function(Wg_f2 + Uh_f2), axis=0)\n",
        "      F_ = tf.concat((F_, f_i), axis=0)\n",
        "    o_t = self.recurrent_function(Wg_o2 + Uh_o2)\n",
        "\n",
        "    c_g = F_[self.seq_len + 1, :] * c_tm1_g + tf.math.reduce_sum((F_[:self.seq_len + 1, :] * c_tm1[:self.seq_len + 1, :]), axis=0, keepdims=True)\n",
        "    g_t = o_t * self.activation_fn(c_g)\n",
        "\n",
        "    H = tf.concat((H[:self.seq_len + 1, :], g_t,), axis=0)\n",
        "    c = tf.concat((c[:self.seq_len + 1, :], c_g,), axis=0)\n",
        "      \n",
        "    return inputs, [H, c], g_t"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e0ea09444657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSLSTMcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   def __init__(self, units=100, window=3, use_bias=True, recurrent_fn=sigmoid, activation_fn=tanh, seq_len=max_len, \n\u001b[1;32m      3\u001b[0m                \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                bias_initializer='zeros', bias_regularizer=None, bias_constraint=None):\n\u001b[1;32m      5\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSLSTMcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk8BgqnkwdWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "units = 100\n",
        "\n",
        "slstm = SLSTMcell(units=units)\n",
        "H = tf.zeros(shape=(max_len+1, embedding_dim))\n",
        "c = tf.zeros(shape=(max_len+1, units))\n",
        "states = [tf.identity(H), tf.identity(c)]\n",
        "for t in range(9):\n",
        "  output, states, g_ = slstm(output, states)\n",
        "\n",
        "\n",
        "\n",
        "# units = 32\n",
        "# init_state = tf.zeros(shape=(1, (max_len+2)*2))\n",
        "# states = tf.identity(init_state)\n",
        "# for i in range(max_len):\n",
        "#     states = slstm(output, states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnymqSBVwLIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = tf.zeros((2,100))\n",
        "b = tf.zeros((1,100))\n",
        "tf.concat((a,b), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBYr5GjenrSp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3912009e-ed4a-4ec2-b90c-b176e42bf4ea"
      },
      "source": [
        "p = tf.constant([1,2,3], shape=(1,3))\n",
        "p"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[1, 2, 3]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxeEzeEjmned",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "4279330a-3bac-4bc3-a317-21e2cf015c11"
      },
      "source": [
        "prova = tf.constant([])\n",
        "tf.stack((prova, p))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-293cb8f28a37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprova\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprova\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m       \u001b[0;31m# If the input is a constant list, it can be converted to a constant op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Input list contains non-constant tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1500\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cast_nested_seqs_to_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1502\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_autopacking_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"packed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1406\u001b[0m     \u001b[0;31m# checking.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_or_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_or_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m   \u001b[0mmust_pack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m   \u001b[0mconverted_elems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   6457\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6458\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6459\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6461\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute Pack as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:Pack] name: stack"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEhljSs37uFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "41e8ece1-8ffc-4d23-af58-8e763d4fc5f5"
      },
      "source": [
        "x = tf.constant([1,2,3], shape=(1,3))\n",
        "W = tf.constant([[1,2],[3,4],[5,6]])\n",
        "W.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-60db4551d329>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z7espd4QRob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "ca1aa675-86d5-4ae1-b626-143853db2a2e"
      },
      "source": [
        "tf.reduce_mean(W[:,:-1], axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3a529ef656ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX6oVb1XO_KI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "6272bd60-292f-4a64-915c-8079aee23496"
      },
      "source": [
        "tf.concat((W[:,0:1],W), axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
              "array([[1, 1, 2],\n",
              "       [3, 3, 4],\n",
              "       [5, 5, 6]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOeiJ4G0GvGS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2cc3e81-9a10-480a-868c-d93502ec147c"
      },
      "source": [
        "K.dot(x,W)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[22, 28]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXwigKi5G0DN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = 2*tf.identity(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8M4aU3QIdKQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b6f6b4d-1301-4e24-aa95-d11bc25917c3"
      },
      "source": [
        "tf.concat((x,y), axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[1, 2, 3, 2, 4, 6]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2u4WOTtIepH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "642d668a-16e8-4e80-d8dc-cf807d8d7a86"
      },
      "source": [
        "W[2:,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[5, 6]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRXhTQmGtXJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Softmax_classification(keras.layer.Layer):\n",
        "  def __init__(self, axis=-1, kernel_initializer='uniform', kernel_regularizer=None, kernel_constraint=None,\n",
        "               bias_initializer='zeros', bias_regularizer=None, bias_constraint=None, **kwargs):\n",
        "    super(Softmax, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    input_dim = input_shape[-1]\n",
        "    self.W = self.add_weight(shape=(input_dim, ??#classification output),\n",
        "                                  name='W',\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  regularizer=self.kernel_regularizer,\n",
        "                                  constraint=self.kernel_constraint)\n",
        "    self.bias = self.add_weight(shape=(self.units * 10,),\n",
        "                                    name='bias',\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    regularizer=self.bias_regularizer,\n",
        "                                    constraint=self.bias_constraint)\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    inputs = inputs[2]\n",
        "    Wg = K.dot(inputs, W)\n",
        "    Wg = K.bias_add(Wg, self.bias)\n",
        "    return K.softmax(Wg, axis=self.axis)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}